# Ph√¢n T√≠ch Chi Ti·∫øt Ki·∫øn Tr√∫c H·ªá Th·ªëng Real-Time Stock Analytics

## üìë M·ª•c L·ª•c

1. [T·ªïng Quan H·ªá Th·ªëng](#1-t·ªïng-quan-h·ªá-th·ªëng)
2. [Ki·∫øn Tr√∫c T·ªïng Th·ªÉ](#2-ki·∫øn-tr√∫c-t·ªïng-th·ªÉ)
3. [Ph√¢n T√≠ch Chi Ti·∫øt T·ª´ng Layer](#3-ph√¢n-t√≠ch-chi-ti·∫øt-t·ª´ng-layer)
4. [Data Flow & Processing Patterns](#4-data-flow--processing-patterns)
5. [Infrastructure & Deployment](#5-infrastructure--deployment)
6. [Design Patterns & Best Practices](#6-design-patterns--best-practices)

---

## 1. T·ªïng Quan H·ªá Th·ªëng

### 1.1 M·ª•c ƒê√≠ch H·ªá Th·ªëng

H·ªá th·ªëng ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ:
- **Thu th·∫≠p real-time**: L·∫•y d·ªØ li·ªáu ch·ª©ng kho√°n t·ª´ Finnhub API m·ªói 6 gi√¢y
- **X·ª≠ l√Ω streaming**: S·ª≠ d·ª•ng Kafka ƒë·ªÉ x·ª≠ l√Ω messages real-time
- **L∆∞u tr·ªØ ph√¢n t·∫ßng**: √Åp d·ª•ng Medallion Architecture (Bronze-Silver-Gold)
- **Ph√¢n t√≠ch d·ªØ li·ªáu**: T·∫°o c√°c metrics v√† KPIs cho dashboard

### 1.2 Ki·∫øn Tr√∫c T·ªïng Th·ªÉ

H·ªá th·ªëng tu√¢n theo **ki·∫øn tr√∫c event-driven** v·ªõi c√°c ƒë·∫∑c ƒëi·ªÉm:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    EXTERNAL DATA SOURCE                     ‚îÇ
‚îÇ                    (Finnhub API)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DATA INGESTION LAYER                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ  Producer    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Kafka     ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  (Python)    ‚îÇ         ‚îÇ  (Broker)    ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                                  ‚îÇ                          ‚îÇ
‚îÇ                                  ‚ñº                          ‚îÇ
‚îÇ                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ                          ‚îÇ   Consumer   ‚îÇ                   ‚îÇ
‚îÇ                          ‚îÇ   (Python)   ‚îÇ                   ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              STORAGE LAYER                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ    MinIO     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  BigQuery    ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  (S3-like)   ‚îÇ         ‚îÇ  (Bronze)    ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         TRANSFORMATION LAYER (dbt)                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   Bronze     ‚îÇ‚îÄ‚ñ∂‚îÇ   Silver     ‚îÇ‚îÄ‚ñ∂‚îÇ    Gold      ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   (View)     ‚îÇ  ‚îÇ  (Incremental)‚îÇ  ‚îÇ   (Tables)   ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ANALYTICS & VISUALIZATION                      ‚îÇ
‚îÇ         (BigQuery Gold Tables + Dashboards)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.3 Technology Stack

| Category | Technology | Version/Purpose |
|----------|-----------|-----------------|
| **Message Broker** | Apache Kafka | 7.4.1 - Event streaming |
| **Coordination** | Apache Zookeeper | 7.4.1 - Kafka coordination |
| **Object Storage** | MinIO | Latest - S3-compatible storage |
| **Orchestration** | Apache Airflow | 2.9.3 - Workflow management |
| **Database** | PostgreSQL | 15 - Airflow metadata |
| **Data Warehouse** | Google BigQuery | Cloud - Analytics engine |
| **Transformation** | dbt | Latest - SQL transformation |
| **Monitoring** | Kafdrop, Grafana | Latest - Observability |
| **Language** | Python | 3.12 - Application code |

---

## 2. Ki·∫øn Tr√∫c T·ªïng Th·ªÉ

### 2.1 High-Level Architecture

H·ªá th·ªëng ƒë∆∞·ª£c chia th√†nh **5 t·∫ßng ch√≠nh**:

#### **T·∫ßng 1: Data Ingestion (Real-Time)**
- **Producer**: Fetch data t·ª´ API v√† publish v√†o Kafka
- **Kafka**: Message broker v·ªõi topic `stock-quotes`
- **Consumer**: Consume messages v√† l∆∞u v√†o MinIO

#### **T·∫ßng 2: Batch Processing (Scheduled)**
- **Airflow DAG**: Ch·∫°y m·ªói 1 ph√∫t ƒë·ªÉ load incremental data
- **MinIO ‚Üí BigQuery**: ETL process v·ªõi metadata tracking

#### **T·∫ßng 3: Data Warehouse (Bronze)**
- **BigQuery Raw Tables**: L∆∞u tr·ªØ d·ªØ li·ªáu th√¥
- **Metadata Table**: Track last processed timestamp

#### **T·∫ßng 4: Data Transformation (Silver & Gold)**
- **dbt Bronze**: View ƒë·ªÉ rename columns
- **dbt Silver**: Incremental table v·ªõi validation
- **dbt Gold**: Aggregated tables cho analytics

#### **T·∫ßng 5: Analytics & Consumption**
- **Gold Tables**: KPI, candlestick, history
- **Dashboards**: Visualization tools

### 2.2 Data Flow Patterns

#### **Pattern 1: Real-Time Streaming (6 gi√¢y)**
```
Finnhub API 
  ‚Üí Producer (fetch_quote)
  ‚Üí Kafka Topic (stock-quotes)
  ‚Üí Consumer (consume & save)
  ‚Üí MinIO (bronze-transactions/{symbol}/{ts}.json)
```

**ƒê·∫∑c ƒëi·ªÉm:**
- **Latency**: ~6 gi√¢y end-to-end
- **Throughput**: 5 symbols √ó 10 calls/min = 50 messages/ph√∫t
- **Reliability**: Kafka ƒë·∫£m b·∫£o message kh√¥ng m·∫•t (consumer group)

#### **Pattern 2: Batch Processing (1 ph√∫t)**
```
MinIO (new files)
  ‚Üí Airflow DAG (download_from_minio)
  ‚Üí Local temp storage
  ‚Üí Airflow DAG (load_bigquery)
  ‚Üí BigQuery Bronze (bronze_stock_quotes_raw)
  ‚Üí Metadata update (metadata_last_ts)
```

**ƒê·∫∑c ƒëi·ªÉm:**
- **Incremental**: Ch·ªâ load files m·ªõi (d·ª±a tr√™n timestamp)
- **Parallel**: X·ª≠ l√Ω 5 symbols song song
- **Idempotent**: Metadata tracking tr√°nh duplicate

#### **Pattern 3: Data Transformation (On-demand ho·∫∑c scheduled)**
```
BigQuery Bronze (raw)
  ‚Üí dbt Bronze (view - rename)
  ‚Üí dbt Silver (incremental - validate & dedupe)
  ‚Üí dbt Gold (tables - aggregate)
```

**ƒê·∫∑c ƒëi·ªÉm:**
- **Incremental Strategy**: Merge cho Silver v√† Gold
- **Data Quality**: Validation ·ªü Silver layer
- **Performance**: Ch·ªâ transform d·ªØ li·ªáu m·ªõi

---

## 3. Ph√¢n T√≠ch Chi Ti·∫øt T·ª´ng Layer

### 3.1 Data Ingestion Layer

#### 3.1.1 Producer Component

**File**: `infra/producer/producer.py`

**Ch·ª©c nƒÉng:**
```python
# 1. Kh·ªüi t·∫°o Kafka Producer
producer = KafkaProducer(
    bootstrap_servers=["localhost:29092"],
    value_serializer=lambda v: json.dumps(v).encode("utf-8")
)

# 2. Fetch data t·ª´ API
def fetch_quote(symbol):
    url = f"{BASE_URL}?symbol={symbol}&token={API_KEY}"
    response = requests.get(url)
    data = response.json()
    data["symbol"] = symbol
    data["fetched_at"] = int(time.time())  # Th√™m timestamp
    return data

# 3. Loop v√† publish
while True:
    for symbol in SYMBOLS:
        quote = fetch_quote(symbol)
        producer.send("stock-quotes", value=quote)
    time.sleep(6)  # Rate limiting
```

**Ph√¢n t√≠ch:**

1. **Serialization**: 
   - Python dict ‚Üí JSON string ‚Üí UTF-8 bytes
   - Kafka l∆∞u d∆∞·ªõi d·∫°ng binary

2. **Rate Limiting**:
   - 5 symbols √ó 10 calls/min = 50 calls/min
   - Finnhub limit: 60 calls/min
   - Sleep 6 gi√¢y gi·ªØa c√°c batch

3. **Error Handling**:
   - Try-catch trong `fetch_quote()`
   - Continue n·∫øu m·ªôt symbol fail

4. **Data Enrichment**:
   - Th√™m `symbol` v√†o response
   - Th√™m `fetched_at` (epoch timestamp)

**Message Format:**
```json
{
  "c": 150.25,           // current price
  "d": 2.50,             // change amount
  "dp": 1.69,            // change percent
  "h": 152.00,           // day high
  "l": 148.50,           // day low
  "o": 149.00,           // day open
  "pc": 147.75,          // previous close
  "t": 1704067200,       // market timestamp (epoch)
  "symbol": "AAPL",      // added by producer
  "fetched_at": 1704067256  // added by producer
}
```

#### 3.1.2 Kafka Infrastructure

**Configuration** (`docker-compose.yml`):

```yaml
kafka:
  image: confluentinc/cp-kafka:7.4.1
  environment:
    KAFKA_BROKER_ID: 1
    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
```

**Ph√¢n t√≠ch:**

1. **Listeners**:
   - `PLAINTEXT://0.0.0.0:9092`: Internal (container-to-container)
   - `PLAINTEXT_HOST://0.0.0.0:29092`: External (host-to-container)

2. **Replication**:
   - `REPLICATION_FACTOR: 1`: Single broker (dev environment)
   - Production n√™n d√πng >= 3

3. **Topic**: `stock-quotes`
   - Partitions: Default (c√≥ th·ªÉ scale)
   - Retention: Default (7 days)

#### 3.1.3 Consumer Component

**File**: `infra/consumer/consumer.py`

**Ch·ª©c nƒÉng:**
```python
# 1. Kh·ªüi t·∫°o MinIO client
s3 = boto3.client(
    "s3",
    endpoint_url="http://localhost:9002",
    aws_access_key_id="admin",
    aws_secret_access_key="password123"
)

# 2. Kh·ªüi t·∫°o Kafka Consumer
consumer = KafkaConsumer(
    "stock-quotes",
    bootstrap_servers=["localhost:29092"],
    enable_auto_commit=True,
    auto_offset_reset="earliest",
    group_id="bronze-Consumer",
    value_deserializer=lambda v: json.loads(v.decode("utf-8"))
)

# 3. Consume v√† save
for message in consumer:
    record = message.value
    symbol = record.get("symbol")
    ts = record.get("fetched_at", int(time.time()))
    key = f"{symbol}/{ts}.json"
    
    s3.put_object(
        Bucket="bronze-transactions",
        Key=key,
        Body=json.dumps(record),
        ContentType="application/json"
    )
```

**Ph√¢n t√≠ch:**

1. **Consumer Group**: `bronze-Consumer`
   - ƒê·∫£m b·∫£o m·ªói message ch·ªâ ƒë∆∞·ª£c consume 1 l·∫ßn
   - Auto-commit offset sau khi process

2. **Offset Management**:
   - `auto_offset_reset="earliest"`: ƒê·ªçc t·ª´ ƒë·∫ßu n·∫øu ch∆∞a c√≥ offset
   - `enable_auto_commit=True`: T·ª± ƒë·ªông commit sau khi process

3. **Storage Strategy**:
   - Key pattern: `{symbol}/{timestamp}.json`
   - Partitioned by symbol (d·ªÖ query theo symbol)
   - Timestamp-based naming (d·ªÖ sort v√† filter)

4. **Idempotency**:
   - M·ªói message t·∫°o 1 file unique (symbol + timestamp)
   - Kh√¥ng c√≥ risk duplicate n·∫øu consumer restart

### 3.2 Storage Layer

#### 3.2.1 MinIO Configuration

**Docker Compose:**
```yaml
minio:
  image: minio/minio:latest
  ports:
    - "9001:9001"  # Console UI
    - "9002:9000"  # S3 API
  environment:
    MINIO_ROOT_USER: admin
    MINIO_ROOT_PASSWORD: password123
  command: server /data --console-address ":9001"
  volumes:
    - minio_data:/data
```

**Ph√¢n t√≠ch:**

1. **S3-Compatible API**:
   - S·ª≠ d·ª•ng Boto3 (AWS SDK)
   - T∆∞∆°ng th√≠ch 100% v·ªõi S3 API
   - C√≥ th·ªÉ migrate sang AWS S3 d·ªÖ d√†ng

2. **Storage Structure**:
   ```
   bronze-transactions/
   ‚îú‚îÄ‚îÄ AAPL/
   ‚îÇ   ‚îú‚îÄ‚îÄ 1704067200.json
   ‚îÇ   ‚îú‚îÄ‚îÄ 1704067260.json
   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îú‚îÄ‚îÄ MSFT/
   ‚îÇ   ‚îî‚îÄ‚îÄ ...
   ‚îî‚îÄ‚îÄ ...
   ```

3. **Use Case**:
   - **Temporary storage**: L∆∞u raw data tr∆∞·ªõc khi load v√†o BigQuery
   - **Backup**: C√≥ th·ªÉ gi·ªØ l·∫°i ƒë·ªÉ replay n·∫øu c·∫ßn
   - **Buffering**: Decouple Kafka consumer v√† BigQuery loader

#### 3.2.2 BigQuery Bronze Layer

**Table Schema**: `bronze_stock_quotes_raw`

**Structure** (t·ª´ JSON):
```sql
CREATE TABLE `real-time-stock-analytics-25.stock.bronze_stock_quotes_raw` (
  c FLOAT64,           -- current_price
  d FLOAT64,           -- change_amount
  dp FLOAT64,          -- change_percent
  h FLOAT64,           -- day_high
  l FLOAT64,           -- day_low
  o FLOAT64,           -- day_open
  pc FLOAT64,          -- prev_close
  t INT64,             -- market_timestamp
  symbol STRING,       -- stock symbol
  fetched_at INT64     -- fetch timestamp
)
```

**Metadata Table**: `metadata_last_ts`
```sql
CREATE TABLE `real-time-stock-analytics-25.stock.metadata_last_ts` (
  symbol STRING,
  last_ts INT64
)
```

**Ph√¢n t√≠ch:**

1. **Incremental Loading Strategy**:
   ```python
   # 1. ƒê·ªçc last_ts t·ª´ metadata
   last_ts = get_last_ts(symbol)  # Query BigQuery
   
   # 2. Filter files m·ªõi h∆°n last_ts
   if ts > last_ts:
       download_and_load()
   
   # 3. Update metadata sau khi load
   update_last_ts(symbol, max_ts)
   ```

2. **MERGE Statement** (idempotent):
   ```sql
   MERGE `metadata_last_ts` T
   USING (SELECT 'AAPL' AS symbol, 1704067260 AS last_ts) S
   ON T.symbol = S.symbol
   WHEN MATCHED THEN UPDATE SET last_ts = S.last_ts
   WHEN NOT MATCHED THEN INSERT (symbol, last_ts) VALUES (S.symbol, S.last_ts)
   ```

3. **Pagination Handling**:
   ```python
   def list_all_objects(s3, bucket, prefix):
       objects = []
       token = None
       while True:
           if token:
               resp = s3.list_objects_v2(..., ContinuationToken=token)
           else:
               resp = s3.list_objects_v2(...)
           objects.extend(resp.get("Contents", []))
           if not resp.get("IsTruncated"):
               break
           token = resp["NextContinuationToken"]
       return objects
   ```
   - S3 API ch·ªâ tr·∫£ v·ªÅ t·ªëi ƒëa 1000 objects/l·∫ßn
   - C·∫ßn pagination v·ªõi `ContinuationToken`

### 3.3 Transformation Layer (dbt)

#### 3.3.1 Bronze Layer

**File**: `dbt_stocks/models/bronze/bronze_stock_quotes.sql`

```sql
{{ config(
    materialized = 'view',
    schema = 'bronze'
) }}

SELECT
  c AS current_price,
  d AS change_amount,
  dp AS change_percent,
  h AS day_high,
  l AS day_low,
  o AS day_open,
  pc AS prev_close,
  t AS market_timestamp,
  symbol,
  fetched_at
FROM {{ source('raw', 'bronze_stock_quotes_raw') }}
```

**Ph√¢n t√≠ch:**

1. **Materialization: VIEW**
   - Kh√¥ng l∆∞u tr·ªØ d·ªØ li·ªáu
   - Ch·ªâ l√† SQL transformation
   - Real-time query t·ª´ raw table

2. **Column Renaming**:
   - T·ª´ t√™n ng·∫Øn (c, d, dp) ‚Üí t√™n c√≥ √Ω nghƒ©a
   - D·ªÖ ƒë·ªçc v√† maintain

3. **Source Definition** (`sources.yml`):
   ```yaml
   sources:
     - name: raw
       database: real-time-stock-analytics-25
       schema: stock
       tables:
         - name: bronze_stock_quotes_raw
   ```

#### 3.3.2 Silver Layer

**File**: `dbt_stocks/models/silver/silver_stock_quotes.sql`

**C·∫•u tr√∫c:**

```sql
{{ config(
    materialized = 'incremental',
    schema = 'silver',
    unique_key = ['symbol', 'market_timestamp_raw'],
    incremental_strategy = 'merge'
) }}

WITH base AS (
    -- Type casting
    SELECT
        symbol,
        CAST(current_price AS FLOAT64) AS current_price,
        CAST(market_timestamp AS INT64) AS market_timestamp_raw,
        TIMESTAMP_SECONDS(CAST(market_timestamp AS INT64)) AS market_time_utc,
        DATETIME(TIMESTAMP_SECONDS(...), "America/New_York") AS market_time_us,
        TIMESTAMP_SECONDS(CAST(fetched_at AS INT64)) AS fetched_at
    FROM {{ ref('bronze_stock_quotes') }}
),

filtered AS (
    -- Incremental filter
    SELECT b.*
    FROM base b
    {% if is_incremental() %}
    LEFT JOIN {{ this }} s
      ON s.symbol = b.symbol
      AND s.market_timestamp_raw = b.market_timestamp_raw
    WHERE s.symbol IS NULL
    {% endif %}
),

dedup AS (
    -- Remove duplicates trong batch
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY symbol, market_timestamp_raw
            ORDER BY fetched_at DESC
        ) AS rn
    FROM filtered
),

validated AS (
    -- Data quality checks
    SELECT *
    FROM dedup
    WHERE rn = 1
      AND current_price > 0
      AND prev_close > 0
      AND day_open > 0
      AND day_high >= day_low
)

SELECT ... FROM validated
```

**Ph√¢n t√≠ch chi ti·∫øt:**

1. **Incremental Strategy: MERGE**
   - Ch·ªâ process d·ªØ li·ªáu m·ªõi
   - `is_incremental()` macro ki·ªÉm tra l·∫ßn ch·∫°y ƒë·∫ßu ti√™n hay kh√¥ng
   - LEFT JOIN ƒë·ªÉ filter records ƒë√£ t·ªìn t·∫°i

2. **Type Casting**:
   - String/Number ‚Üí FLOAT64
   - Epoch seconds ‚Üí TIMESTAMP
   - Timezone conversion (UTC ‚Üí US Eastern)

3. **Deduplication**:
   - Window function `ROW_NUMBER()` partition by `(symbol, market_timestamp_raw)`
   - Order by `fetched_at DESC` (l·∫•y b·∫£n m·ªõi nh·∫•t)
   - Filter `rn = 1`

4. **Data Validation**:
   - `current_price > 0`: Gi√° ph·∫£i d∆∞∆°ng
   - `prev_close > 0`: Gi√° ƒë√≥ng c·ª≠a tr∆∞·ªõc ph·∫£i d∆∞∆°ng
   - `day_open > 0`: Gi√° m·ªü c·ª≠a ph·∫£i d∆∞∆°ng
   - `day_high >= day_low`: Logic check (high kh√¥ng th·ªÉ < low)

5. **Performance**:
   - Incremental ch·ªâ scan d·ªØ li·ªáu m·ªõi
   - MERGE statement hi·ªáu qu·∫£ h∆°n DELETE + INSERT
   - Index tr√™n `unique_key` ƒë·ªÉ tƒÉng t·ªëc JOIN

#### 3.3.3 Gold Layer

**5 Models ch√≠nh:**

##### **1. gold_kpi.sql** (Latest KPI)
```sql
{{ config(materialized='table', schema='gold') }}

WITH ranked AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY symbol
            ORDER BY fetched_at DESC
        ) AS rn
    FROM {{ ref('silver_stock_quotes') }}
)
SELECT ... FROM ranked WHERE rn = 1
```

**M·ª•c ƒë√≠ch**: L·∫•y KPI m·ªõi nh·∫•t cho m·ªói symbol (d·ª±a tr√™n `fetched_at`)

##### **2. gold_kpi_latest.sql** (View t·ª´ History)
```sql
{{ config(materialized='view', schema='gold') }}

WITH ranked AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY symbol
            ORDER BY market_time_utc DESC
        ) AS rn
    FROM {{ ref('gold_kpi_history') }}
)
SELECT ... FROM ranked WHERE rn = 1
```

**M·ª•c ƒë√≠ch**: Latest KPI t·ª´ history table (d·ª±a tr√™n `market_time_utc`)

##### **3. gold_kpi_history.sql** (Incremental History)
```sql
{{ config(
    materialized='incremental',
    schema='gold',
    unique_key=['symbol', 'market_time_utc'],
    incremental_strategy='merge'
) }}

-- Incremental filter t∆∞∆°ng t·ª± Silver
```

**M·ª•c ƒë√≠ch**: L∆∞u to√†n b·ªô l·ªãch s·ª≠ KPI theo th·ªùi gian

##### **4. gold_candlestick.sql** (OHLC Aggregation)
```sql
WITH enriched AS (
    SELECT
        symbol,
        CAST(market_time_us AS DATE) AS candle_date,
        -- Window functions ƒë·ªÉ t√≠nh OPEN v√† CLOSE
        FIRST_VALUE(current_price) OVER (
            PARTITION BY symbol, CAST(market_time_us AS DATE)
            ORDER BY market_time_us
            ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
        ) AS candle_open,
        LAST_VALUE(current_price) OVER (...) AS candle_close
    FROM {{ ref('silver_stock_quotes') }}
),
daily AS (
    SELECT
        symbol,
        candle_date,
        MIN(day_low) AS candle_low,
        MAX(day_high) AS candle_high,
        ANY_VALUE(candle_open) AS candle_open,
        ANY_VALUE(candle_close) AS candle_close,
        AVG(current_price) AS trend_line
    FROM enriched
    GROUP BY symbol, candle_date
),
ranked AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY symbol
            ORDER BY candle_date DESC
        ) AS rn
    FROM daily
)
SELECT ... FROM ranked WHERE rn <= 12
```

**Ph√¢n t√≠ch:**

1. **Window Functions**:
   - `FIRST_VALUE()`: Gi√° ƒë·∫ßu ti√™n trong ng√†y (OPEN)
   - `LAST_VALUE()`: Gi√° cu·ªëi c√πng trong ng√†y (CLOSE)
   - `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`: To√†n b·ªô partition

2. **Aggregation**:
   - `MIN(day_low)`: Low c·ªßa ng√†y
   - `MAX(day_high)`: High c·ªßa ng√†y
   - `AVG(current_price)`: Trend line

3. **Filtering**:
   - Ch·ªâ l·∫•y 12 ng√†y g·∫ßn nh·∫•t (`rn <= 12`)
   - ƒê·ªß ƒë·ªÉ hi·ªÉn th·ªã candlestick chart

##### **5. gold_treechart.sql** (Volatility Analysis)
```sql
WITH source AS (
    SELECT symbol, current_price AS price, market_time_us
    FROM {{ ref('silver_stock_quotes') }}
),
latest_day AS (
    SELECT MAX(CAST(market_time_us AS DATE)) AS max_day
    FROM source
),
latest_avg_price AS (
    SELECT symbol, AVG(price) AS avg_price
    FROM source s
    JOIN latest_day ld ON CAST(s.market_time_us AS DATE) = ld.max_day
    GROUP BY symbol
),
volatility AS (
    SELECT
        symbol,
        STDDEV_POP(price) AS volatility,
        STDDEV_POP(price) / NULLIF(AVG(price), 0) AS relative_volatility
    FROM source
    GROUP BY symbol
)
SELECT a.symbol, a.avg_price, v.volatility, v.relative_volatility
FROM latest_avg_price a
JOIN volatility v USING(symbol)
```

**Ph√¢n t√≠ch:**

1. **Volatility Calculation**:
   - `STDDEV_POP()`: Population standard deviation
   - `relative_volatility`: Volatility / Average price (normalized)

2. **Latest Day Average**:
   - L·∫•y gi√° trung b√¨nh c·ªßa ng√†y m·ªõi nh·∫•t
   - D√πng cho tree chart visualization

### 3.4 Orchestration Layer (Airflow)

#### 3.4.1 DAG Structure

**File**: `infra/dags/minio_to_bigquery_multi.py`

```python
with DAG(
    dag_id="minio_to_bigquery_multi",
    schedule_interval="* * * * *",  # Every 1 minute
    catchup=False,
) as dag:
    
    for symbol in SYMBOLS:
        t1 = PythonOperator(
            task_id=f"download_{symbol}",
            python_callable=download_from_minio,
            op_kwargs={"symbol": symbol},
        )
        
        t2 = PythonOperator(
            task_id=f"load_{symbol}",
            python_callable=load_bigquery,
            op_kwargs={"symbol": symbol},
        )
        
        t1 >> t2  # Sequential dependency
```

**Task Graph:**
```
download_AAPL ‚îÄ‚îÄ‚ñ∂ load_AAPL
download_MSFT ‚îÄ‚îÄ‚ñ∂ load_MSFT
download_TSLA ‚îÄ‚îÄ‚ñ∂ load_TSLA
download_GOOGL ‚îÄ‚îÄ‚ñ∂ load_GOOGL
download_AMZN ‚îÄ‚îÄ‚ñ∂ load_AMZN
```

**Ph√¢n t√≠ch:**

1. **Parallel Execution**:
   - 5 symbols ch·∫°y song song (kh√¥ng ph·ª• thu·ªôc nhau)
   - Airflow scheduler t·ª± ƒë·ªông parallelize

2. **Sequential per Symbol**:
   - `download_{symbol}` ph·∫£i ho√†n th√†nh tr∆∞·ªõc `load_{symbol}`
   - XCom truy·ªÅn danh s√°ch files gi·ªØa tasks

3. **XCom Communication**:
   ```python
   # Task 1: Push data
   context["ti"].xcom_push(key=f"{symbol}_files", value=new_files)
   context["ti"].xcom_push(key=f"{symbol}_max_ts", value=max_ts)
   
   # Task 2: Pull data
   files = context["ti"].xcom_pull(key=f"{symbol}_files")
   max_ts = context["ti"].xcom_pull(key=f"{symbol}_max_ts")
   ```

4. **Error Handling**:
   ```python
   default_args = {
       "retries": 1,
       "retry_delay": timedelta(minutes=2),
   }
   ```
   - Retry 1 l·∫ßn n·∫øu fail
   - Delay 2 ph√∫t tr∆∞·ªõc khi retry

#### 3.4.2 Airflow Infrastructure

**Docker Compose Configuration:**

```yaml
airflow-webserver:
  build: .
  image: airflow-custom:latest
  environment:
    GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/airflow_gcp_key.json
    GCP_PROJECT: "real-time-stock-analytics-25"
  volumes:
    - ./dags:/opt/airflow/dags
    - ./airflow_gcp_key.json:/opt/airflow/airflow_gcp_key.json

airflow-scheduler:
  image: airflow-custom:latest
  environment:
    GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/airflow_gcp_key.json
    GCP_PROJECT: "real-time-stock-analytics-25"
```

**Ph√¢n t√≠ch:**

1. **Custom Image**:
   - Build t·ª´ `apache/airflow:2.9.3`
   - Install th√™m packages t·ª´ `requirements.txt`
   - BigQuery client libraries

2. **GCP Authentication**:
   - Mount service account key v√†o container
   - Set `GOOGLE_APPLICATION_CREDENTIALS` environment variable
   - BigQuery client t·ª± ƒë·ªông authenticate

3. **Volume Mounts**:
   - `./dags`: DAG files (hot reload)
   - `./logs`: Task logs
   - `./airflow_gcp_key.json`: GCP credentials

---

## 4. Data Flow & Processing Patterns

### 4.1 End-to-End Data Flow

#### **Timeline c·ªßa m·ªôt message:**

```
T=0s:    Producer fetch t·ª´ Finnhub API
T=0.1s:  Producer publish v√†o Kafka
T=0.2s:  Consumer consume t·ª´ Kafka
T=0.3s:  Consumer save v√†o MinIO (AAPL/1704067200.json)
T=60s:   Airflow DAG trigger (schedule)
T=60.1s: Airflow download file t·ª´ MinIO
T=60.2s: Airflow load v√†o BigQuery Bronze
T=60.3s: Airflow update metadata_last_ts
T=61s:   dbt run (n·∫øu schedule ho·∫∑c manual)
T=61.1s: dbt Bronze view query
T=61.2s: dbt Silver incremental merge
T=61.3s: dbt Gold tables update
```

**Total Latency**: ~61-62 gi√¢y t·ª´ API ƒë·∫øn Gold layer

### 4.2 Processing Patterns

#### **Pattern 1: Event-Driven (Real-Time)**
- **Trigger**: Producer fetch API m·ªói 6 gi√¢y
- **Processing**: Asynchronous, non-blocking
- **Storage**: MinIO (temporary buffer)

#### **Pattern 2: Batch Processing (Scheduled)**
- **Trigger**: Cron schedule (`* * * * *`)
- **Processing**: Synchronous, sequential per symbol
- **Storage**: BigQuery (persistent)

#### **Pattern 3: Incremental Processing**
- **Strategy**: Timestamp-based filtering
- **Benefits**: 
  - Ch·ªâ process d·ªØ li·ªáu m·ªõi
  - Gi·∫£m cost v√† latency
  - Idempotent (c√≥ th·ªÉ retry)

#### **Pattern 4: Lambda Architecture**
- **Real-time path**: Kafka ‚Üí MinIO (low latency)
- **Batch path**: MinIO ‚Üí BigQuery (high accuracy)
- **Unified view**: dbt Gold layer

### 4.3 Data Consistency

#### **At-Least-Once Delivery**
- Kafka: Consumer group ƒë·∫£m b·∫£o message ƒë∆∞·ª£c process
- MinIO: File-based storage (idempotent)
- BigQuery: MERGE statement (idempotent)

#### **Deduplication Strategy**
1. **Kafka Level**: Consumer group (m·ªói message 1 l·∫ßn)
2. **MinIO Level**: File naming (symbol + timestamp = unique)
3. **BigQuery Level**: 
   - Metadata tracking (timestamp-based)
   - dbt Silver: ROW_NUMBER() deduplication
   - dbt Gold: Unique key constraints

---

## 5. Infrastructure & Deployment

### 5.1 Container Architecture

**Services v√† Dependencies:**

```
postgres (Airflow metadata)
  ‚Üë
  ‚îú‚îÄ‚îÄ airflow-init (one-time setup)
  ‚îú‚îÄ‚îÄ airflow-webserver
  ‚îî‚îÄ‚îÄ airflow-scheduler

zookeeper
  ‚Üë
  ‚îî‚îÄ‚îÄ kafka
      ‚Üë
      ‚îî‚îÄ‚îÄ kafdrop

minio (standalone)

grafana (standalone)
```

### 5.2 Network Architecture

**Internal Network (Docker)**
- Services giao ti·∫øp qua service names
- Example: `http://minio:9000`, `kafka:9092`

**External Access (Host)**
- Port mapping ƒë·ªÉ access t·ª´ host
- Example: `localhost:29092`, `localhost:9002`

### 5.3 Storage Volumes

```yaml
volumes:
  postgres_data:    # Airflow metadata persistence
  minio_data:       # Object storage persistence
  grafana_data:     # Grafana dashboards persistence
```

**Ph√¢n t√≠ch:**
- Named volumes persist data khi container restart
- Data kh√¥ng b·ªã m·∫•t khi `docker-compose down`

### 5.4 Deployment Process

**1. Build Custom Airflow Image:**
```bash
cd infra
docker build -t airflow-custom:latest .
```

**2. Start Services:**
```bash
docker-compose up -d
```

**3. Verify:**
- Airflow UI: http://localhost:8080
- Kafdrop: http://localhost:9000
- MinIO Console: http://localhost:9001
- Grafana: http://localhost:3000

**4. Run Producer & Consumer:**
```bash
# Terminal 1
python infra/producer/producer.py

# Terminal 2
python infra/consumer/consumer.py
```

**5. Run dbt:**
```bash
cd dbt_stocks
dbt run
```

---

## 6. Design Patterns & Best Practices

### 6.1 Medallion Architecture

**Bronze (Raw)**
- Purpose: Store raw data as-is
- Format: JSON files, raw tables
- No transformation

**Silver (Cleaned)**
- Purpose: Cleaned, validated, deduplicated data
- Format: Incremental tables
- Transformations: Type casting, validation, deduplication

**Gold (Curated)**
- Purpose: Business-ready analytics tables
- Format: Aggregated tables, views
- Transformations: Aggregations, KPIs, business logic

**Benefits:**
- Clear separation of concerns
- Easy to debug (trace data lineage)
- Flexible (c√≥ th·ªÉ rebuild t·ª´ b·∫•t k·ª≥ layer n√†o)

### 6.2 Incremental Processing

**Pattern:**
```python
# 1. Read checkpoint
last_ts = get_last_ts(symbol)

# 2. Filter new data
new_data = filter(lambda x: x.ts > last_ts, all_data)

# 3. Process new data
process(new_data)

# 4. Update checkpoint
update_last_ts(symbol, max_ts)
```

**Benefits:**
- Efficiency: Ch·ªâ process d·ªØ li·ªáu m·ªõi
- Cost: Gi·∫£m compute v√† storage cost
- Scalability: Handle large datasets

### 6.3 Idempotency

**Strategy:**
1. **Unique Keys**: `(symbol, timestamp)` ƒë·∫£m b·∫£o uniqueness
2. **MERGE Statements**: Upsert thay v√¨ INSERT
3. **Metadata Tracking**: Track processed timestamps
4. **Deduplication**: ROW_NUMBER() trong dbt

**Benefits:**
- Safe to retry
- No duplicate data
- Consistent results

### 6.4 Error Handling

**Layers:**

1. **Producer**:
   ```python
   try:
       quote = fetch_quote(symbol)
   except Exception as e:
       print(f"Error: {e}")
       continue  # Skip v√† ti·∫øp t·ª•c
   ```

2. **Consumer**:
   - Kafka auto-commit: Message ƒë∆∞·ª£c mark l√† processed
   - N·∫øu consumer crash, message s·∫Ω ƒë∆∞·ª£c retry

3. **Airflow**:
   - Retry logic: `retries=1, retry_delay=2min`
   - Task failure ‚Üí Airflow UI alert

4. **dbt**:
   - SQL errors ‚Üí dbt logs
   - Incremental filter ‚Üí Safe to rerun

### 6.5 Monitoring & Observability

**Tools:**

1. **Kafdrop**: 
   - Monitor Kafka topics
   - View messages
   - Check consumer lag

2. **Airflow UI**:
   - DAG runs status
   - Task logs
   - XCom values

3. **Grafana**:
   - Custom dashboards
   - Metrics visualization

4. **BigQuery Console**:
   - Query performance
   - Storage usage
   - Job history

### 6.6 Scalability Considerations

**Current Limitations:**
- Single Kafka broker (dev environment)
- Single Airflow scheduler
- No horizontal scaling

**Production Recommendations:**

1. **Kafka**:
   - Multi-broker cluster (3+ brokers)
   - Increase partitions
   - Replication factor >= 3

2. **Airflow**:
   - Celery executor (distributed)
   - Multiple workers
   - Redis/RabbitMQ as message broker

3. **BigQuery**:
   - Partition tables by date
   - Cluster by symbol
   - Use streaming inserts for real-time

4. **dbt**:
   - Parallel model execution
   - Incremental models (ƒë√£ implement)
   - Materialized views cho frequently queried data

---

## 7. K·∫øt Lu·∫≠n

### 7.1 ƒêi·ªÉm M·∫°nh

1. **Architecture**: 
   - Event-driven + Batch processing
   - Medallion Architecture
   - Clear separation of concerns

2. **Reliability**:
   - Idempotent operations
   - Error handling
   - Metadata tracking

3. **Performance**:
   - Incremental processing
   - Parallel execution
   - Efficient data structures

4. **Maintainability**:
   - Modular design
   - Clear data lineage
   - Well-documented

### 7.2 C·∫£i Thi·ªán Ti·ªÅm NƒÉng

1. **Real-Time Processing**:
   - Th√™m Kafka Streams ho·∫∑c Flink
   - Real-time aggregation

2. **Data Quality**:
   - Great Expectations integration
   - Automated data quality checks

3. **Monitoring**:
   - Prometheus + Grafana
   - Alerting rules

4. **Testing**:
   - Unit tests cho Python code
   - dbt tests cho data quality

5. **Documentation**:
   - Data dictionary
   - API documentation
   - Runbooks

---

**T√†i li·ªáu n√†y cung c·∫•p ph√¢n t√≠ch chi ti·∫øt t·ª´ t·ªïng quan ƒë·∫øn implementation c·ªßa h·ªá th·ªëng Real-Time Stock Analytics.**

