# Docker Deployment - H∆∞·ªõng D·∫´n Ng·∫Øn G·ªçn

## üöÄ Quick Start

### 1. Build Custom Airflow Image

```bash
cd infra
docker build -t airflow-custom:latest .
```

**Dockerfile l√†m g√¨:**
- Base image: `apache/airflow:2.9.3`
- Install packages t·ª´ `requirements.txt` (boto3, google-cloud-bigquery, ...)
- T·∫°o image `airflow-custom:latest` ƒë·ªÉ d√πng cho webserver v√† scheduler

### 2. Start T·∫•t C·∫£ Services

```bash
docker-compose up -d
```

**Services ƒë∆∞·ª£c start:**
- **Zookeeper** ‚Üí **Kafka** ‚Üí **Kafdrop** (Kafka ecosystem)
- **MinIO** (Object storage)
- **PostgreSQL** (Airflow metadata)
- **Airflow Init** (One-time: init DB + create admin user)
- **Airflow Webserver** (UI: http://localhost:8080)
- **Airflow Scheduler** (Ch·∫°y DAGs)
- **Grafana** (Monitoring: http://localhost:3000)

### 3. Ki·ªÉm Tra Services

```bash
# Xem status
docker-compose ps

# Xem logs
docker-compose logs -f airflow-scheduler
docker-compose logs -f kafka
```

## üìã Services Overview

| Service | Port | Purpose | Access |
|---------|------|---------|--------|
| **Kafka** | 29092 | Message broker | `localhost:29092` |
| **Kafdrop** | 9000 | Kafka UI | http://localhost:9000 |
| **MinIO Console** | 9001 | MinIO UI | http://localhost:9001 |
| **MinIO API** | 9002 | S3 API | `localhost:9002` |
| **PostgreSQL** | 5432 | Airflow DB | `localhost:5432` |
| **Airflow** | 8080 | Airflow UI | http://localhost:8080 |
| **Grafana** | 3000 | Dashboards | http://localhost:3000 |

## üîë Key Points

### **Dependencies (Start Order)**
```
postgres ‚Üí airflow-init ‚Üí airflow-webserver ‚Üí airflow-scheduler
zookeeper ‚Üí kafka ‚Üí kafdrop
```

### **Volumes (Persistent Data)**
- `postgres_data`: Airflow metadata
- `minio_data`: Object storage files
- `grafana_data`: Grafana dashboards

### **Volume Mounts (Hot Reload)**
- `./dags` ‚Üí `/opt/airflow/dags` (DAG files)
- `./airflow_gcp_key.json` ‚Üí `/opt/airflow/airflow_gcp_key.json` (GCP credentials)
- `./logs` ‚Üí `/opt/airflow/logs` (Task logs)

### **Environment Variables**
- `GOOGLE_APPLICATION_CREDENTIALS`: Path to GCP service account key
- `GCP_PROJECT`: BigQuery project ID
- `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN`: PostgreSQL connection string

## üõ†Ô∏è Common Commands

```bash
# Start services
docker-compose up -d

# Stop services
docker-compose down

# Stop v√† x√≥a volumes (‚ö†Ô∏è m·∫•t data)
docker-compose down -v

# Rebuild Airflow image
docker-compose build airflow-webserver airflow-scheduler

# Restart m·ªôt service
docker-compose restart airflow-scheduler

# Xem logs real-time
docker-compose logs -f [service-name]

# Execute command trong container
docker-compose exec airflow-webserver bash
```

## ‚ö†Ô∏è L∆∞u √ù

1. **Airflow Init**: Ch·∫°y 1 l·∫ßn duy nh·∫•t khi start l·∫ßn ƒë·∫ßu
2. **GCP Credentials**: File `airflow_gcp_key.json` ph·∫£i t·ªìn t·∫°i trong `infra/`
3. **Port Conflicts**: ƒê·∫£m b·∫£o c√°c ports (8080, 9000, 29092, ...) kh√¥ng b·ªã conflict
4. **Memory**: Kafka v√† Airflow c·∫ßn ƒë·ªß RAM (recommend 4GB+)

## üîÑ Workflow Sau Khi Deploy

1. **Start Producer** (external):
   ```bash
   python infra/producer/producer.py
   ```

2. **Start Consumer** (external):
   ```bash
   python infra/consumer/consumer.py
   ```

3. **Airflow DAG**: T·ª± ƒë·ªông ch·∫°y m·ªói 1 ph√∫t (schedule: `* * * * *`)

4. **dbt Transform** (manual ho·∫∑c schedule):
   ```bash
   cd dbt_stocks
   dbt run
   ```

## üêõ Troubleshooting

**Airflow kh√¥ng start:**
```bash
# Check logs
docker-compose logs airflow-webserver

# Rebuild image
docker-compose build airflow-webserver
docker-compose up -d airflow-webserver
```

**Kafka connection error:**
```bash
# Check Kafka status
docker-compose logs kafka

# Restart Kafka
docker-compose restart kafka
```

**BigQuery authentication error:**
- Ki·ªÉm tra file `airflow_gcp_key.json` c√≥ t·ªìn t·∫°i
- Ki·ªÉm tra environment variable `GOOGLE_APPLICATION_CREDENTIALS`

