# üöÄ Roadmap C·∫£i Thi·ªán H·ªá Th·ªëng Real-Time Stock Analytics

## üìã M·ª•c L·ª•c

1. [C·∫£i Thi·ªán Ngay L·∫≠p T·ª©c (Quick Wins)](#1-c·∫£i-thi·ªán-ngay-l·∫≠p-t·ª©c-quick-wins)
2. [C·∫£i Thi·ªán Code Quality & Reliability](#2-c·∫£i-thi·ªán-code-quality--reliability)
3. [C·∫£i Thi·ªán Performance & Scalability](#3-c·∫£i-thi·ªán-performance--scalability)
4. [C·∫£i Thi·ªán Monitoring & Observability](#4-c·∫£i-thi·ªán-monitoring--observability)
5. [C·∫£i Thi·ªán Data Quality & Testing](#5-c·∫£i-thi·ªán-data-quality--testing)
6. [C·∫£i Thi·ªán Security & Configuration](#6-c·∫£i-thi·ªán-security--configuration)
7. [Advanced Features](#7-advanced-features)

---

## 1. C·∫£i Thi·ªán Ngay L·∫≠p T·ª©c (Quick Wins)

### 1.1 Environment Variables & Configuration Management

**V·∫•n ƒë·ªÅ hi·ªán t·∫°i:**
- Hardcode credentials trong code (API keys, passwords)
- Kh√≥ qu·∫£n l√Ω cho nhi·ªÅu m√¥i tr∆∞·ªùng (dev, staging, prod)

**Gi·∫£i ph√°p:**
```python
# producer.py
import os
from dotenv import load_dotenv

load_dotenv()

API_KEY = os.getenv("FINNHUB_API_KEY")
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:29092")
SYMBOLS = os.getenv("STOCK_SYMBOLS", "AAPL,MSFT,TSLA,GOOGL,AMZN").split(",")
```

**File `.env`:**
```env
FINNHUB_API_KEY=your_api_key_here
KAFKA_BOOTSTRAP_SERVERS=localhost:29092
MINIO_ENDPOINT=http://localhost:9002
MINIO_ACCESS_KEY=admin
MINIO_SECRET_KEY=password123
BQ_PROJECT=real-time-stock-analytics-25
BQ_DATASET=stock
STOCK_SYMBOLS=AAPL,MSFT,TSLA,GOOGL,AMZN
```

**L·ª£i √≠ch:**
- ‚úÖ B·∫£o m·∫≠t t·ªët h∆°n (kh√¥ng commit credentials)
- ‚úÖ D·ªÖ qu·∫£n l√Ω nhi·ªÅu m√¥i tr∆∞·ªùng
- ‚úÖ D·ªÖ thay ƒë·ªïi config kh√¥ng c·∫ßn s·ª≠a code

### 1.2 Logging & Error Handling

**V·∫•n ƒë·ªÅ hi·ªán t·∫°i:**
- Ch·ªâ d√πng `print()` statements
- Kh√¥ng c√≥ structured logging
- Error handling c∆° b·∫£n

**Gi·∫£i ph√°p:**
```python
import logging
from datetime import datetime

# Setup logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/producer_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def fetch_quote(symbol):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        logger.info(f"Successfully fetched {symbol}", extra={"symbol": symbol, "price": data.get("c")})
        return data
    except requests.exceptions.Timeout:
        logger.error(f"Timeout fetching {symbol}", exc_info=True)
        return None
    except requests.exceptions.RequestException as e:
        logger.error(f"Error fetching {symbol}: {e}", exc_info=True)
        return None
```

**L·ª£i √≠ch:**
- ‚úÖ D·ªÖ debug v·ªõi structured logs
- ‚úÖ C√≥ th·ªÉ t√≠ch h·ª£p v·ªõi log aggregation tools
- ‚úÖ Better error tracking

### 1.3 Producer Callback & Error Handling

**V·∫•n ƒë·ªÅ hi·ªán t·∫°i:**
- Kh√¥ng bi·∫øt message c√≥ ƒë∆∞·ª£c g·ª≠i th√†nh c√¥ng kh√¥ng
- Kh√¥ng c√≥ retry logic

**Gi·∫£i ph√°p:**
```python
from kafka.errors import KafkaError

def on_send_success(record_metadata):
    logger.info(f"Message sent to topic={record_metadata.topic}, "
                f"partition={record_metadata.partition}, "
                f"offset={record_metadata.offset}")

def on_send_error(exception):
    logger.error(f"Failed to send message: {exception}", exc_info=True)

producer = KafkaProducer(
    bootstrap_servers=[KAFKA_BOOTSTRAP_SERVERS],
    value_serializer=lambda v: json.dumps(v).encode("utf-8"),
    retries=3,
    acks='all',  # Wait for all replicas
    max_in_flight_requests_per_connection=1
)

# Send with callbacks
future = producer.send("stock-quotes", value=quote)
future.add_callback(on_send_success)
future.add_errback(on_send_error)
```

### 1.4 Consumer Error Handling & Dead Letter Queue

**V·∫•n ƒë·ªÅ hi·ªán t·∫°i:**
- N·∫øu MinIO fail, message b·ªã m·∫•t
- Kh√¥ng c√≥ retry mechanism

**Gi·∫£i ph√°p:**
```python
from kafka import KafkaConsumer
import time

MAX_RETRIES = 3
RETRY_DELAY = 5

def save_to_minio_with_retry(record, max_retries=MAX_RETRIES):
    for attempt in range(max_retries):
        try:
            s3.put_object(
                Bucket=bucket_name,
                Key=key,
                Body=json.dumps(record),
                ContentType="application/json"
            )
            logger.info(f"Successfully saved {key}")
            return True
        except Exception as e:
            logger.warning(f"Attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(RETRY_DELAY)
            else:
                # Send to dead letter topic
                send_to_dlq(record, str(e))
                return False
    return False

# Dead Letter Queue producer
dlq_producer = KafkaProducer(...)

def send_to_dlq(record, error):
    dlq_producer.send("stock-quotes-dlq", value={
        "original_record": record,
        "error": error,
        "timestamp": int(time.time())
    })
```

---

## 2. C·∫£i Thi·ªán Code Quality & Reliability

### 2.1 Code Structure & Modularization

**V·∫•n ƒë·ªÅ hi·ªán t·∫°i:**
- Code flat, kh√¥ng c√≥ structure
- Kh√≥ test v√† maintain

**Gi·∫£i ph√°p:**
```
infra/
‚îú‚îÄ‚îÄ producer/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ producer.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ api_client.py      # Finnhub API client
‚îÇ   ‚îî‚îÄ‚îÄ kafka_client.py    # Kafka producer wrapper
‚îú‚îÄ‚îÄ consumer/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ consumer.py
‚îÇ   ‚îú‚îÄ‚îÄ storage.py          # MinIO operations
‚îÇ   ‚îî‚îÄ‚îÄ kafka_client.py
‚îî‚îÄ‚îÄ common/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ logger.py           # Logging setup
    ‚îî‚îÄ‚îÄ exceptions.py       # Custom exceptions
```

**Example:**
```python
# api_client.py
class FinnhubClient:
    def __init__(self, api_key, base_url):
        self.api_key = api_key
        self.base_url = base_url
        self.session = requests.Session()
    
    def get_quote(self, symbol):
        url = f"{self.base_url}?symbol={symbol}&token={self.api_key}"
        response = self.session.get(url, timeout=10)
        response.raise_for_status()
        return response.json()
```

### 2.2 Unit Tests

**T·∫°o test files:**
```python
# tests/test_producer.py
import pytest
from unittest.mock import Mock, patch
from producer.api_client import FinnhubClient

def test_fetch_quote_success():
    client = FinnhubClient("test_key", "https://test.api")
    with patch('requests.Session.get') as mock_get:
        mock_get.return_value.json.return_value = {"c": 150.0}
        mock_get.return_value.raise_for_status = Mock()
        
        result = client.get_quote("AAPL")
        assert result["c"] == 150.0

def test_fetch_quote_timeout():
    client = FinnhubClient("test_key", "https://test.api")
    with patch('requests.Session.get') as mock_get:
        mock_get.side_effect = requests.exceptions.Timeout()
        
        with pytest.raises(requests.exceptions.Timeout):
            client.get_quote("AAPL")
```

**Setup pytest:**
```bash
pip install pytest pytest-cov
pytest tests/ --cov=producer --cov-report=html
```

### 2.3 Type Hints & Documentation

**C·∫£i thi·ªán code:**
```python
from typing import Optional, Dict, Any
from dataclasses import dataclass

@dataclass
class StockQuote:
    symbol: str
    current_price: float
    change_amount: float
    change_percent: float
    day_high: float
    day_low: float
    day_open: float
    prev_close: float
    market_timestamp: int
    fetched_at: int

def fetch_quote(symbol: str) -> Optional[StockQuote]:
    """
    Fetch stock quote from Finnhub API.
    
    Args:
        symbol: Stock symbol (e.g., 'AAPL')
    
    Returns:
        StockQuote object if successful, None otherwise
    
    Raises:
        requests.exceptions.RequestException: If API call fails
    """
    # Implementation
    pass
```

### 2.4 Airflow DAG Improvements

**V·∫•n ƒë·ªÅ hi·ªán t·∫°i:**
- Hardcode SQL trong Python
- Kh√¥ng c√≥ data validation
- Error handling c∆° b·∫£n

**Gi·∫£i ph√°p:**
```python
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator

def create_symbol_task_group(symbol: str, dag: DAG) -> TaskGroup:
    with TaskGroup(group_id=f"process_{symbol}", dag=dag) as tg:
        download_task = PythonOperator(
            task_id=f"download_{symbol}",
            python_callable=download_from_minio,
            op_kwargs={"symbol": symbol},
        )
        
        validate_task = PythonOperator(
            task_id=f"validate_{symbol}",
            python_callable=validate_data,
            op_kwargs={"symbol": symbol},
        )
        
        load_task = BigQueryInsertJobOperator(
            task_id=f"load_{symbol}",
            configuration={
                "load": {
                    "destinationTable": {
                        "projectId": BQ_PROJECT,
                        "datasetId": BQ_DATASET,
                        "tableId": BQ_TABLE,
                    },
                    "sourceFormat": "NEWLINE_DELIMITED_JSON",
                }
            },
        )
        
        download_task >> validate_task >> load_task
        return tg

# Main DAG
with DAG(...) as dag:
    for symbol in SYMBOLS:
        create_symbol_task_group(symbol, dag)
```

---

## 3. C·∫£i Thi·ªán Performance & Scalability

### 3.1 Kafka Producer Batching

**C·∫£i thi·ªán throughput:**
```python
producer = KafkaProducer(
    bootstrap_servers=[KAFKA_BOOTSTRAP_SERVERS],
    value_serializer=lambda v: json.dumps(v).encode("utf-8"),
    batch_size=16384,  # 16KB batches
    linger_ms=10,      # Wait 10ms to batch
    compression_type='gzip',  # Compress messages
    acks='all',
    retries=3
)
```

### 3.2 Consumer Batching

**Process multiple messages:**
```python
from kafka import KafkaConsumer
from collections import deque

BATCH_SIZE = 100
BATCH_TIMEOUT = 5  # seconds

def process_batch(messages):
    """Process batch of messages to MinIO"""
    for message in messages:
        # Save to MinIO
        pass

messages_batch = deque()
last_batch_time = time.time()

for message in consumer:
    messages_batch.append(message)
    
    if len(messages_batch) >= BATCH_SIZE or \
       (time.time() - last_batch_time) >= BATCH_TIMEOUT:
        process_batch(messages_batch)
        messages_batch.clear()
        last_batch_time = time.time()
        consumer.commit()
```

### 3.3 BigQuery Streaming Inserts

**Thay v√¨ batch insert:**
```python
from google.cloud import bigquery

def stream_insert_bigquery(rows):
    client = bigquery.Client()
    table = client.get_table(f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}")
    
    errors = client.insert_rows_json(table, rows)
    if errors:
        logger.error(f"BigQuery errors: {errors}")
        raise Exception(f"BigQuery insert failed: {errors}")
```

### 3.4 dbt Incremental Improvements

**Partition v√† cluster:**
```sql
-- silver_stock_quotes.sql
{{ config(
    materialized='incremental',
    partition_by={'field': 'market_time_utc', 'data_type': 'timestamp'},
    cluster_by=['symbol'],
    unique_key=['symbol', 'market_timestamp_raw'],
    incremental_strategy='merge'
) }}
```

**L·ª£i √≠ch:**
- ‚úÖ Faster queries (partition pruning)
- ‚úÖ Lower cost (scan less data)
- ‚úÖ Better performance cho time-based queries

### 3.5 Airflow Parallel Processing

**S·ª≠ d·ª•ng Dynamic Task Mapping:**
```python
from airflow import DAG
from airflow.decorators import task

@task
def download_from_minio(symbol: str):
    # Download logic
    return files

@task
def load_bigquery(files: list):
    # Load logic
    pass

with DAG(...) as dag:
    symbols = ["AAPL", "MSFT", "TSLA", "GOOGL", "AMZN"]
    
    files = download_from_minio.expand(symbol=symbols)
    load_bigquery.expand(files=files)
```

---

## 4. C·∫£i Thi·ªán Monitoring & Observability

### 4.1 Metrics v·ªõi Prometheus

**Th√™m metrics:**
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# Metrics
messages_produced = Counter('kafka_messages_produced_total', 'Total messages produced', ['symbol'])
messages_consumed = Counter('kafka_messages_consumed_total', 'Total messages consumed')
api_request_duration = Histogram('api_request_duration_seconds', 'API request duration')
api_errors = Counter('api_errors_total', 'Total API errors', ['symbol', 'error_type'])
queue_size = Gauge('kafka_queue_size', 'Kafka queue size')

# In producer
messages_produced.labels(symbol=symbol).inc()
api_request_duration.observe(duration)

# Start metrics server
start_http_server(8000)  # Expose metrics on port 8000
```

**Docker Compose th√™m:**
```yaml
prometheus:
  image: prom/prometheus:latest
  ports:
    - "9090:9090"
  volumes:
    - ./prometheus.yml:/etc/prometheus/prometheus.yml

grafana:
  # Already exists, add Prometheus as data source
```

### 4.2 Structured Logging v·ªõi JSON

**JSON logs:**
```python
import json
import logging

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        if hasattr(record, "extra_fields"):
            log_entry.update(record.extra_fields)
        return json.dumps(log_entry)

logger = logging.getLogger()
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)
```

### 4.3 Health Checks

**Health check endpoints:**
```python
from flask import Flask, jsonify

app = Flask(__name__)

@app.route('/health')
def health():
    return jsonify({
        "status": "healthy",
        "kafka": check_kafka_connection(),
        "minio": check_minio_connection(),
        "bigquery": check_bigquery_connection()
    })

@app.route('/metrics')
def metrics():
    return jsonify({
        "messages_produced": get_produced_count(),
        "messages_consumed": get_consumed_count(),
        "queue_size": get_queue_size()
    })
```

### 4.4 Airflow Monitoring

**Custom Airflow operators v·ªõi metrics:**
```python
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults

class MonitoredPythonOperator(BaseOperator):
    @apply_defaults
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
    
    def execute(self, context):
        start_time = time.time()
        try:
            result = self.python_callable(**context)
            duration = time.time() - start_time
            log_metric("task_success", 1, tags={"task_id": self.task_id})
            log_metric("task_duration", duration, tags={"task_id": self.task_id})
            return result
        except Exception as e:
            log_metric("task_failure", 1, tags={"task_id": self.task_id})
            raise
```

---

## 5. C·∫£i Thi·ªán Data Quality & Testing

### 5.1 dbt Tests

**Th√™m tests:**
```yaml
# models/silver/schema.yml
version: 2

models:
  - name: silver_stock_quotes
    columns:
      - name: current_price
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 10000
      
      - name: symbol
        tests:
          - not_null
          - accepted_values:
              values: ['AAPL', 'MSFT', 'TSLA', 'GOOGL', 'AMZN']
      
      - name: day_high
        tests:
          - dbt_utils.expression_is_true:
              expression: "day_high >= day_low"
```

**Run tests:**
```bash
dbt test
```

### 5.2 Great Expectations Integration

**Data quality checks:**
```python
import great_expectations as ge

def validate_silver_data():
    df = get_silver_data()
    
    # Create expectation suite
    suite = ge.dataset.PandasDataset(df)
    
    # Expectations
    suite.expect_column_values_to_be_between("current_price", 0, 10000)
    suite.expect_column_values_to_not_be_null("symbol")
    suite.expect_column_pair_values_A_to_be_greater_than_B("day_high", "day_low")
    
    # Validate
    results = suite.validate()
    if not results["success"]:
        send_alert(results)
```

### 5.3 Data Lineage Tracking

**dbt lineage:**
```bash
# Generate lineage graph
dbt docs generate
dbt docs serve

# View at http://localhost:8080
```

**Custom lineage tracking:**
```python
def track_data_lineage(source, target, transformation):
    lineage_entry = {
        "timestamp": datetime.now().isoformat(),
        "source": source,
        "target": target,
        "transformation": transformation,
        "rows_processed": get_row_count(target)
    }
    save_to_lineage_table(lineage_entry)
```

### 5.4 Anomaly Detection

**Detect anomalies:**
```sql
-- dbt model: silver_anomalies.sql
WITH stats AS (
    SELECT
        symbol,
        AVG(current_price) AS avg_price,
        STDDEV(current_price) AS stddev_price
    FROM {{ ref('silver_stock_quotes') }}
    WHERE market_time_utc >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)
    GROUP BY symbol
),
current AS (
    SELECT
        symbol,
        current_price,
        market_time_utc
    FROM {{ ref('silver_stock_quotes') }}
    WHERE market_time_utc >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)
)
SELECT
    c.symbol,
    c.current_price,
    s.avg_price,
    ABS(c.current_price - s.avg_price) / s.stddev_price AS z_score
FROM current c
JOIN stats s USING(symbol)
WHERE ABS(c.current_price - s.avg_price) / s.stddev_price > 3  -- 3 sigma
```

---

## 6. C·∫£i Thi·ªán Security & Configuration

### 6.1 Secrets Management

**S·ª≠ d·ª•ng Docker Secrets ho·∫∑c Vault:**
```yaml
# docker-compose.yml
services:
  producer:
    secrets:
      - finnhub_api_key
      - kafka_password

secrets:
  finnhub_api_key:
    file: ./secrets/finnhub_api_key.txt
  kafka_password:
    file: ./secrets/kafka_password.txt
```

**Ho·∫∑c s·ª≠ d·ª•ng AWS Secrets Manager / GCP Secret Manager:**
```python
from google.cloud import secretmanager

def get_secret(secret_id):
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{PROJECT_ID}/secrets/{secret_id}/versions/latest"
    response = client.access_secret_version(request={"name": name})
    return response.payload.data.decode("UTF-8")

API_KEY = get_secret("finnhub-api-key")
```

### 6.2 Network Security

**Docker network isolation:**
```yaml
networks:
  kafka_network:
    driver: bridge
  airflow_network:
    driver: bridge

services:
  kafka:
    networks:
      - kafka_network
  
  airflow-scheduler:
    networks:
      - airflow_network
      - kafka_network  # Only if needed
```

### 6.3 IAM & RBAC

**BigQuery IAM:**
```python
# Service account v·ªõi least privilege
# Ch·ªâ c√≥ quy·ªÅn:
# - bigquery.jobs.create
# - bigquery.tables.getData
# - bigquery.tables.updateData
```

**Airflow RBAC:**
```python
# airflow.cfg
[webserver]
rbac = True

# T·∫°o roles v√† permissions
```

### 6.4 Configuration Management

**S·ª≠ d·ª•ng ConfigMap pattern:**
```python
# config.py
from dataclasses import dataclass
from typing import List

@dataclass
class KafkaConfig:
    bootstrap_servers: List[str]
    topic: str
    group_id: str

@dataclass
class MinIOConfig:
    endpoint: str
    access_key: str
    secret_key: str
    bucket: str

@dataclass
class AppConfig:
    kafka: KafkaConfig
    minio: MinIOConfig
    symbols: List[str]

def load_config() -> AppConfig:
    return AppConfig(
        kafka=KafkaConfig(...),
        minio=MinIOConfig(...),
        symbols=os.getenv("SYMBOLS").split(",")
    )
```

---

## 7. Advanced Features

### 7.1 Real-Time Aggregation v·ªõi Kafka Streams

**Kafka Streams application:**
```python
from kafka import KafkaProducer, KafkaConsumer
from kafka.streams import KafkaStreams
import json

def aggregate_realtime():
    builder = KafkaStreams.builder()
    
    # Source stream
    source = builder.stream("stock-quotes")
    
    # Aggregate by symbol
    aggregated = source.group_by_key().aggregate(
        initializer=lambda: {"count": 0, "sum": 0.0, "min": float('inf'), "max": 0.0},
        aggregator=lambda key, value, aggregate: {
            "count": aggregate["count"] + 1,
            "sum": aggregate["sum"] + value["c"],
            "min": min(aggregate["min"], value["c"]),
            "max": max(aggregate["max"], value["c"])
        },
        windows=TimeWindows.of(60000)  # 1 minute windows
    )
    
    # Sink to new topic
    aggregated.to("stock-quotes-aggregated")
    
    streams = builder.build()
    streams.start()
```

### 7.2 Caching Layer

**Redis cache cho API calls:**
```python
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def get_quote_with_cache(symbol, cache_ttl=60):
    cache_key = f"quote:{symbol}"
    
    # Check cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Fetch from API
    quote = fetch_quote(symbol)
    
    # Cache result
    if quote:
        redis_client.setex(cache_key, cache_ttl, json.dumps(quote))
    
    return quote
```

### 7.3 Alerting System

**Alert khi c√≥ v·∫•n ƒë·ªÅ:**
```python
from airflow.operators.email import EmailOperator
from airflow.operators.slack_operator import SlackAPIPostOperator

def create_alert_task(dag):
    return SlackAPIPostOperator(
        task_id='send_alert',
        token=os.getenv('SLACK_TOKEN'),
        channel='#alerts',
        text='Data pipeline failed!',
        dag=dag
    )

# Ho·∫∑c d√πng PagerDuty, Opsgenie, etc.
```

### 7.4 Data Backfill & Recovery

**Backfill script:**
```python
def backfill_data(start_date, end_date, symbols):
    """Backfill historical data"""
    current_date = start_date
    while current_date <= end_date:
        for symbol in symbols:
            # Fetch historical data
            data = fetch_historical_quote(symbol, current_date)
            if data:
                # Process and load
                process_and_load(data)
        current_date += timedelta(days=1)
```

### 7.5 Multi-Environment Support

**Environment-specific configs:**
```yaml
# config/dev.yml
kafka:
  bootstrap_servers: ["localhost:29092"]
minio:
  endpoint: "http://localhost:9002"

# config/prod.yml
kafka:
  bootstrap_servers: ["kafka-prod-1:9092", "kafka-prod-2:9092"]
minio:
  endpoint: "https://s3.amazonaws.com"
```

### 7.6 CI/CD Pipeline

**GitHub Actions workflow:**
```yaml
# .github/workflows/deploy.yml
name: Deploy Pipeline

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run tests
        run: pytest tests/
  
  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Build Docker image
        run: docker build -t airflow-custom:latest .
  
  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to production
        run: docker-compose up -d
```

---

## üìä Priority Matrix

| C·∫£i Thi·ªán | Impact | Effort | Priority |
|-----------|--------|--------|----------|
| Environment Variables | High | Low | üî• P0 |
| Logging | High | Low | üî• P0 |
| Error Handling | High | Medium | üî• P0 |
| dbt Tests | Medium | Low | ‚ö° P1 |
| Metrics & Monitoring | High | Medium | ‚ö° P1 |
| Code Structure | Medium | Medium | ‚ö° P1 |
| Unit Tests | Medium | High | üìã P2 |
| Performance Optimization | Medium | Medium | üìã P2 |
| Security Improvements | High | High | üìã P2 |
| Advanced Features | Low | High | üìù P3 |

---

## üéØ Recommended Implementation Order

### **Phase 1 (Week 1-2): Quick Wins**
1. ‚úÖ Environment variables & .env files
2. ‚úÖ Structured logging
3. ‚úÖ Better error handling
4. ‚úÖ Producer/Consumer callbacks

### **Phase 2 (Week 3-4): Quality & Testing**
1. ‚úÖ Code structure & modularization
2. ‚úÖ Unit tests
3. ‚úÖ dbt tests
4. ‚úÖ Type hints

### **Phase 3 (Week 5-6): Monitoring & Observability**
1. ‚úÖ Prometheus metrics
2. ‚úÖ Grafana dashboards
3. ‚úÖ Health checks
4. ‚úÖ Alerting

### **Phase 4 (Week 7-8): Performance & Scale**
1. ‚úÖ Kafka batching
2. ‚úÖ BigQuery partitioning
3. ‚úÖ dbt incremental improvements
4. ‚úÖ Caching layer

### **Phase 5 (Ongoing): Advanced Features**
1. ‚úÖ Real-time aggregation
2. ‚úÖ Data quality automation
3. ‚úÖ CI/CD pipeline
4. ‚úÖ Multi-environment support

---

**T√†i li·ªáu n√†y cung c·∫•p roadmap chi ti·∫øt ƒë·ªÉ c·∫£i thi·ªán h·ªá th·ªëng t·ª´ basic ƒë·∫øn production-ready!**





