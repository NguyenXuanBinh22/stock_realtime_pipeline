# PhÃ¢n TÃ­ch Chi Tiáº¿t: Producer â†’ Consumer & Kafka Topic

## ğŸ“‹ Tá»•ng Quan

Pháº§n nÃ y phÃ¢n tÃ­ch chi tiáº¿t luá»“ng dá»¯ liá»‡u tá»« **Producer** (thu tháº­p tá»« Finnhub API) qua **Kafka Topic** Ä‘áº¿n **Consumer** (lÆ°u vÃ o MinIO), bao gá»“m cáº¥u hÃ¬nh, cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng, vÃ  cÃ¡c Ä‘áº·c Ä‘iá»ƒm ká»¹ thuáº­t.

---

## ğŸ”„ Luá»“ng Dá»¯ Liá»‡u Tá»•ng Thá»ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRODUCER â†’ KAFKA â†’ CONSUMER                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Finnhub    â”‚  HTTP   â”‚   Producer   â”‚  Kafka  â”‚   Kafka     â”‚
â”‚    API      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  producer.py â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Topic:    â”‚
â”‚             â”‚ Request â”‚              â”‚ Message â”‚ stock-quotesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â”‚ Consume
                                                         â–¼
                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                â”‚   Consumer   â”‚
                                                â”‚ consumer.py  â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â”‚ S3 API
                                                       â–¼
                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                â”‚    MinIO     â”‚
                                                â”‚ bronze-trans-â”‚
                                                â”‚  actions/    â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. ğŸš€ PRODUCER (producer.py)

### 1.1 Cáº¥u HÃ¬nh Kafka Producer

**File**: `infra/producer/producer.py`

```python
producer = KafkaProducer(
    bootstrap_servers=["localhost:29092"],
    value_serializer=lambda v: json.dumps(v).encode("utf-8")
)
```

**PhÃ¢n tÃ­ch cáº¥u hÃ¬nh:**

| Tham sá»‘ | GiÃ¡ trá»‹ | MÃ´ táº£ |
|---------|---------|-------|
| `bootstrap_servers` | `["localhost:29092"]` | Kafka broker endpoint (external port) |
| `value_serializer` | `lambda v: json.dumps(v).encode("utf-8")` | Serialize Python dict â†’ JSON string â†’ UTF-8 bytes |

**Chi tiáº¿t:**
- **Bootstrap Server**: `localhost:29092` lÃ  external port Ä‘Æ°á»£c expose tá»« Docker container
- **Serialization Pipeline**: 
  ```
  Python dict â†’ JSON string â†’ UTF-8 bytes â†’ Kafka binary format
  ```
- **Default Settings** (khÃ´ng khai bÃ¡o):
  - `acks=1`: Producer chá» acknowledgment tá»« leader partition
  - `retries=2147483647`: Retry vÃ´ háº¡n (default)
  - `batch_size=16384`: Batch 16KB trÆ°á»›c khi gá»­i
  - `linger_ms=0`: Gá»­i ngay láº­p tá»©c (khÃ´ng Ä‘á»£i batch)

### 1.2 Quy TrÃ¬nh Thu Tháº­p Dá»¯ Liá»‡u

```python
# 1. Fetch tá»« API
def fetch_quote(symbol):
    url = f"{BASE_URL}?symbol={symbol}&token={API_KEY}"
    response = requests.get(url)
    data = response.json()
    data["symbol"] = symbol              # Enrichment
    data["fetched_at"] = int(time.time()) # Enrichment
    return data

# 2. Loop vÃ  publish
while True:
    for symbol in SYMBOLS:  # ["AAPL","MSFT","TSLA","GOOGL","AMZN"]
        quote = fetch_quote(symbol)
        if quote:
            producer.send("stock-quotes", value=quote)
    time.sleep(6)  # Rate limiting
```

**PhÃ¢n tÃ­ch:**

1. **Data Enrichment**:
   - ThÃªm `symbol`: Äá»ƒ consumer biáº¿t message thuá»™c symbol nÃ o
   - ThÃªm `fetched_at`: Timestamp khi fetch (epoch seconds) - dÃ¹ng lÃ m key trong MinIO

2. **Rate Limiting**:
   - **5 symbols** Ã— **10 calls/phÃºt** = **50 calls/phÃºt**
   - Finnhub limit: **60 calls/phÃºt**
   - Sleep **6 giÃ¢y** giá»¯a cÃ¡c batch â†’ **10 batches/phÃºt** = **50 messages/phÃºt**

3. **Error Handling**:
   - Try-catch trong `fetch_quote()` â†’ return `None` náº¿u fail
   - Check `if quote:` trÆ°á»›c khi send â†’ skip náº¿u fetch fail
   - **KhÃ´ng block**: Náº¿u 1 symbol fail, váº«n tiáº¿p tá»¥c vá»›i symbols khÃ¡c

### 1.3 Message Format

**Input tá»« Finnhub API:**
```json
{
  "c": 150.25,    // current price
  "d": 2.50,      // change amount
  "dp": 1.69,     // change percent
  "h": 152.00,    // day high
  "l": 148.50,    // day low
  "o": 149.00,    // day open
  "pc": 147.75,   // previous close
  "t": 1704067200 // market timestamp (epoch)
}
```

**Sau khi Producer enrich:**
```json
{
  "c": 150.25,
  "d": 2.50,
  "dp": 1.69,
  "h": 152.00,
  "l": 148.50,
  "o": 149.00,
  "pc": 147.75,
  "t": 1704067200,
  "symbol": "AAPL",           // â† Added by producer
  "fetched_at": 1704067256    // â† Added by producer
}
```

**Kafka Message Structure:**
- **Topic**: `stock-quotes`
- **Key**: `None` (khÃ´ng partition key â†’ round-robin)
- **Value**: JSON bytes (UTF-8 encoded)
- **Headers**: None
- **Timestamp**: Auto-generated by Kafka

### 1.4 Producer Behavior

**Asynchronous Send:**
- `producer.send()` lÃ  **non-blocking**
- Message Ä‘Æ°á»£c Ä‘Æ°a vÃ o **buffer** vÃ  gá»­i báº¥t Ä‘á»“ng bá»™
- **KhÃ´ng Ä‘á»£i** acknowledgment trÆ°á»›c khi tiáº¿p tá»¥c

**Partitioning Strategy:**
- KhÃ´ng cÃ³ key â†’ **Round-robin** distribution
- 5 symbols â†’ messages Ä‘Æ°á»£c phÃ¢n bá»‘ Ä‘á»u across partitions
- **KhÃ´ng Ä‘áº£m báº£o** messages cá»§a cÃ¹ng symbol vÃ o cÃ¹ng partition

**Throughput:**
- **50 messages/phÃºt** = **~0.83 messages/giÃ¢y**
- **~3000 messages/giá»**
- **~72,000 messages/ngÃ y**

---

## 2. ğŸ“¨ KAFKA TOPIC: `stock-quotes`

### 2.1 Cáº¥u HÃ¬nh Kafka Broker

**File**: `infra/docker-compose.yml`

```yaml
kafka:
  image: confluentinc/cp-kafka:7.4.1
  environment:
    KAFKA_BROKER_ID: 1
    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
```

**PhÃ¢n tÃ­ch Listeners:**

| Listener | Port | Má»¥c Ä‘Ã­ch | Sá»­ dá»¥ng bá»Ÿi |
|----------|------|----------|-------------|
| `PLAINTEXT://0.0.0.0:9092` | 9092 | Internal (container-to-container) | Airflow, services trong Docker network |
| `PLAINTEXT_HOST://0.0.0.0:29092` | 29092 | External (host-to-container) | Producer, Consumer cháº¡y trÃªn host |

**Advertised Listeners:**
- Kafka tráº£ vá» Ä‘á»‹a chá»‰ nÃ y cho clients khi connect
- Internal: `kafka:9092` (DNS trong Docker network)
- External: `localhost:29092` (host machine)

### 2.2 Topic Configuration

**Topic Name**: `stock-quotes`

**Default Settings** (náº¿u khÃ´ng táº¡o explicit):
- **Partitions**: `1` (default)
- **Replication Factor**: `1` (single broker)
- **Retention**: `7 days` (default)
- **Segment Size**: `1GB` (default)
- **Cleanup Policy**: `delete` (default)

**Táº¡o Topic (náº¿u cáº§n):**
```bash
kafka-topics --create \
  --bootstrap-server localhost:29092 \
  --topic stock-quotes \
  --partitions 3 \
  --replication-factor 1
```

**LÆ°u Ã½:**
- **Single Broker**: KhÃ´ng cÃ³ replication â†’ **khÃ´ng fault-tolerant**
- **Single Partition**: KhÃ´ng thá»ƒ scale consumer â†’ **bottleneck**
- **Production nÃªn**: â‰¥3 partitions, â‰¥3 brokers, replication factor â‰¥2

### 2.3 Message Storage

**Kafka Log Structure:**
```
stock-quotes-0/
â”œâ”€â”€ 00000000000000000000.log  (messages)
â”œâ”€â”€ 00000000000000000000.index
â””â”€â”€ 00000000000000000000.timeindex
```

**Message Format trong Log:**
- **Offset**: Sequential number (0, 1, 2, ...)
- **Timestamp**: Auto-generated
- **Key**: `null` (khÃ´ng cÃ³ key)
- **Value**: Binary (JSON bytes)
- **Headers**: Empty

**Retention:**
- Messages Ä‘Æ°á»£c giá»¯ **7 ngÃ y** (default)
- Sau 7 ngÃ y â†’ auto-delete
- Consumer cÃ³ thá»ƒ Ä‘á»c láº¡i tá»« `earliest` offset náº¿u cáº§n

### 2.4 Offset Management

**Consumer Group**: `bronze-Consumer`
- Kafka lÆ°u **offset** cho má»—i consumer group
- Offset Ä‘Æ°á»£c commit vÃ o `__consumer_offsets` topic
- **Replication Factor**: 1 (single broker)

**Offset Storage:**
```
__consumer_offsets/
â”œâ”€â”€ Partition 0: {group_id: "bronze-Consumer", topic: "stock-quotes", offset: 12345}
â””â”€â”€ ...
```

---

## 3. ğŸ“¥ CONSUMER (consumer.py)

### 3.1 Cáº¥u HÃ¬nh Kafka Consumer

**File**: `infra/consumer/consumer.py`

```python
consumer = KafkaConsumer(
    "stock-quotes",
    bootstrap_servers=["localhost:29092"],
    enable_auto_commit=True,
    auto_offset_reset="earliest",
    group_id="bronze-Consumer",
    value_deserializer=lambda v: json.loads(v.decode("utf-8"))
)
```

**PhÃ¢n tÃ­ch cáº¥u hÃ¬nh:**

| Tham sá»‘ | GiÃ¡ trá»‹ | MÃ´ táº£ |
|---------|---------|-------|
| `bootstrap_servers` | `["localhost:29092"]` | Kafka broker endpoint |
| `group_id` | `"bronze-Consumer"` | Consumer group ID (Ä‘áº£m báº£o khÃ´ng máº¥t message) |
| `enable_auto_commit` | `True` | Tá»± Ä‘á»™ng commit offset sau khi consume |
| `auto_offset_reset` | `"earliest"` | Náº¿u khÃ´ng cÃ³ offset â†’ Ä‘á»c tá»« Ä‘áº§u |
| `value_deserializer` | `lambda v: json.loads(v.decode("utf-8"))` | Deserialize: bytes â†’ UTF-8 string â†’ JSON â†’ Python dict |

**Chi tiáº¿t:**

1. **Consumer Group**:
   - **Group ID**: `bronze-Consumer`
   - **Partition Assignment**: Náº¿u 1 consumer â†’ nháº­n táº¥t cáº£ partitions
   - **Load Balancing**: Náº¿u nhiá»u consumers cÃ¹ng group â†’ chia partitions

2. **Offset Management**:
   - **Auto Commit**: `True` â†’ commit sau má»—i `auto.commit.interval.ms` (default: 5s)
   - **Offset Reset**: `earliest` â†’ náº¿u khÃ´ng cÃ³ offset â†’ Ä‘á»c tá»« Ä‘áº§u topic
   - **At-least-once**: CÃ³ thá»ƒ Ä‘á»c láº¡i message náº¿u crash trÆ°á»›c khi commit

3. **Deserialization Pipeline**:
   ```
   Kafka binary â†’ UTF-8 bytes â†’ JSON string â†’ Python dict
   ```

### 3.2 Quy TrÃ¬nh Consume & LÆ°u Trá»¯

```python
# 1. Khá»Ÿi táº¡o MinIO client
s3 = boto3.client(
    "s3",
    endpoint_url="http://localhost:9002",
    aws_access_key_id="admin",
    aws_secret_access_key="password123"
)

# 2. Consume loop
for message in consumer:
    record = message.value  # Python dict (Ä‘Ã£ deserialize)
    symbol = record.get("symbol")
    ts = record.get("fetched_at", int(time.time()))
    key = f"{symbol}/{ts}.json"
    
    # 3. LÆ°u vÃ o MinIO
    s3.put_object(
        Bucket="bronze-transactions",
        Key=key,
        Body=json.dumps(record),
        ContentType="application/json"
    )
```

**PhÃ¢n tÃ­ch:**

1. **Message Processing**:
   - **Blocking Loop**: `for message in consumer` â†’ Ä‘á»£i message má»›i
   - **Synchronous**: Xá»­ lÃ½ tá»«ng message má»™t (khÃ´ng parallel)
   - **Error Handling**: KhÃ´ng cÃ³ try-catch â†’ crash náº¿u cÃ³ lá»—i

2. **MinIO Key Structure**:
   - **Format**: `{symbol}/{timestamp}.json`
   - **VÃ­ dá»¥**: `AAPL/1704067256.json`
   - **Lá»£i Ã­ch**: 
     - Dá»… query theo symbol
     - Timestamp unique â†’ khÃ´ng duplicate
     - Dá»… sort theo thá»i gian

3. **Data Flow**:
   ```
   Kafka Message (bytes)
       â†“ deserialize
   Python dict
       â†“ extract symbol, fetched_at
   MinIO Key: {symbol}/{ts}.json
       â†“ json.dumps
   JSON string
       â†“ S3 API
   MinIO Object Storage
   ```

### 3.3 Consumer Behavior

**Processing Model:**
- **Sequential**: Xá»­ lÃ½ tá»«ng message má»™t
- **Blocking**: Äá»£i MinIO upload xong má»›i tiáº¿p tá»¥c
- **No Batching**: Má»—i message â†’ 1 file riÃªng

**Throughput:**
- **Latency**: ~100-500ms/message (tÃ¹y MinIO)
- **Throughput**: ~2-10 messages/giÃ¢y (náº¿u khÃ´ng bottleneck)
- **Current Load**: 50 messages/phÃºt = ~0.83 messages/giÃ¢y â†’ **khÃ´ng bottleneck**

**Reliability:**
- **At-least-once**: CÃ³ thá»ƒ Ä‘á»c láº¡i message náº¿u crash
- **No Idempotency**: Náº¿u Ä‘á»c láº¡i â†’ táº¡o file duplicate (cÃ¹ng key)
- **Risk**: Náº¿u MinIO fail â†’ message bá»‹ máº¥t (Ä‘Ã£ commit offset)

---

## 4. ğŸ” PhÃ¢n TÃ­ch Chi Tiáº¿t Luá»“ng Dá»¯ Liá»‡u

### 4.1 End-to-End Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STEP-BY-STEP DATA FLOW                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. PRODUCER FETCH
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Finnhub    â”‚ HTTP GET /api/v1/quote?symbol=AAPL&token=xxx
   â”‚    API      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
                                                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Response: {"c": 150.25, "d": 2.50, ..., "t": 1704067200}   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Enrichment: + "symbol": "AAPL"                             â”‚
   â”‚            + "fetched_at": 1704067256                       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. KAFKA PRODUCE
                                                                â”‚
                                                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ producer.send("stock-quotes", value=quote)                  â”‚
   â”‚   â†’ Serialize: dict â†’ JSON â†’ UTF-8 bytes                   â”‚
   â”‚   â†’ Send to Kafka broker (localhost:29092)                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Kafka Topic: stock-quotes                                   â”‚
   â”‚   - Partition: 0 (round-robin)                              â”‚
   â”‚   - Offset: 12345                                           â”‚
   â”‚   - Value: Binary (JSON bytes)                              â”‚
   â”‚   - Timestamp: 1704067256000                                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. KAFKA CONSUME
                                                                â”‚
                                                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ consumer.poll() â†’ Fetch message from Kafka                  â”‚
   â”‚   - Topic: stock-quotes                                     â”‚
   â”‚   - Partition: 0                                            â”‚
   â”‚   - Offset: 12345                                           â”‚
   â”‚   - Value: Binary (JSON bytes)                              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Deserialize: bytes â†’ UTF-8 string â†’ JSON â†’ Python dict     â”‚
   â”‚   â†’ record = {"c": 150.25, "symbol": "AAPL", ...}          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. MINIO STORAGE
                                                                â”‚
                                                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Extract: symbol = "AAPL", ts = 1704067256                  â”‚
   â”‚ Key: "AAPL/1704067256.json"                                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ s3.put_object(                                              â”‚
   â”‚   Bucket="bronze-transactions",                             â”‚
   â”‚   Key="AAPL/1704067256.json",                               â”‚
   â”‚   Body=json.dumps(record)                                   â”‚
   â”‚ )                                                           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ MinIO: bronze-transactions/AAPL/1704067256.json            â”‚
   â”‚   â†’ File stored successfully                                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. OFFSET COMMIT
                                                                â”‚
                                                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Auto-commit offset: 12345                                   â”‚
   â”‚   â†’ Saved to __consumer_offsets topic                      â”‚
   â”‚   â†’ Consumer group: bronze-Consumer                        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Timing & Latency

**Latency Breakdown:**

| Step | Latency | MÃ´ táº£ |
|------|---------|-------|
| API Request | 100-500ms | HTTP GET tá»« Finnhub |
| Producer Send | <10ms | Serialize + buffer |
| Kafka Storage | <10ms | Write to log |
| Consumer Poll | <10ms | Fetch from Kafka |
| Deserialize | <1ms | Bytes â†’ dict |
| MinIO Upload | 50-200ms | S3 API call |
| Offset Commit | <10ms | Write to __consumer_offsets |
| **Total** | **~200-750ms** | End-to-end latency |

**Throughput:**
- **Producer**: 50 messages/phÃºt = **0.83 msg/s**
- **Consumer**: CÃ³ thá»ƒ xá»­ lÃ½ **2-10 msg/s** â†’ **khÃ´ng bottleneck**
- **Buffer**: Kafka buffer messages â†’ consumer cÃ³ thá»ƒ catch up

### 4.3 Error Scenarios

**1. Producer Fail:**
```
Finnhub API Error â†’ fetch_quote() return None â†’ Skip message
â†’ KhÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n messages khÃ¡c
â†’ Continue vá»›i symbols tiáº¿p theo
```

**2. Kafka Broker Down:**
```
producer.send() â†’ ConnectionError
â†’ Producer retry (default: infinite)
â†’ Messages buffer trong memory
â†’ Risk: Memory overflow náº¿u down lÃ¢u
```

**3. Consumer Crash:**
```
Consumer crash â†’ Offset chÆ°a commit
â†’ Auto-commit interval: 5s
â†’ Risk: Re-read last 5s messages (at-least-once)
â†’ MinIO: Duplicate files (cÃ¹ng key â†’ overwrite)
```

**4. MinIO Fail:**
```
s3.put_object() â†’ Error
â†’ Consumer crash (no error handling)
â†’ Offset Ä‘Ã£ commit â†’ Message lost
â†’ Risk: Data loss
```

---

## 5. ğŸ¯ Äiá»ƒm Máº¡nh & Háº¡n Cháº¿

### 5.1 Äiá»ƒm Máº¡nh

1. **Decoupling**:
   - Producer vÃ  Consumer Ä‘á»™c láº­p
   - Kafka lÃ m buffer â†’ khÃ´ng block nhau

2. **Scalability**:
   - CÃ³ thá»ƒ scale producer/consumer Ä‘á»™c láº­p
   - Kafka handle high throughput

3. **Reliability**:
   - Consumer group Ä‘áº£m báº£o khÃ´ng máº¥t message
   - Offset tracking â†’ cÃ³ thá»ƒ resume

4. **Real-time**:
   - Low latency (~200-750ms)
   - Near real-time processing

### 5.2 Háº¡n Cháº¿ & Rá»§i Ro

1. **Single Partition**:
   - KhÃ´ng thá»ƒ scale consumer
   - Bottleneck náº¿u throughput tÄƒng

2. **No Replication**:
   - Single broker â†’ khÃ´ng fault-tolerant
   - Náº¿u Kafka down â†’ máº¥t táº¥t cáº£ messages

3. **Error Handling**:
   - Consumer khÃ´ng cÃ³ try-catch â†’ crash náº¿u MinIO fail
   - Risk: Data loss náº¿u offset Ä‘Ã£ commit

4. **No Idempotency**:
   - Duplicate messages â†’ duplicate files
   - CÃ¹ng key â†’ overwrite (may máº¯n)

5. **No Batching**:
   - Má»—i message â†’ 1 file
   - Inefficient cho MinIO (nhiá»u small files)

---

## 6. ğŸ”§ Khuyáº¿n Nghá»‹ Cáº£i Thiá»‡n

### 6.1 Immediate Improvements

1. **Error Handling trong Consumer**:
```python
for message in consumer:
    try:
        # Process message
        s3.put_object(...)
    except Exception as e:
        print(f"Error processing message: {e}")
        # Log to dead letter queue
        continue  # Skip vÃ  tiáº¿p tá»¥c
```

2. **Manual Offset Commit**:
```python
enable_auto_commit=False
# Commit sau khi upload thÃ nh cÃ´ng
consumer.commit()
```

3. **Partition Key**:
```python
# Producer: DÃ¹ng symbol lÃ m key
producer.send("stock-quotes", key=symbol.encode(), value=quote)
# â†’ Messages cÃ¹ng symbol vÃ o cÃ¹ng partition
```

### 6.2 Production-Ready Improvements

1. **Multiple Partitions**:
   - Táº¡o topic vá»›i 3-5 partitions
   - Scale consumer instances

2. **Replication**:
   - â‰¥3 Kafka brokers
   - Replication factor â‰¥2

3. **Idempotency**:
   - Check file exists trÆ°á»›c khi upload
   - Hoáº·c dÃ¹ng unique key (symbol + timestamp + offset)

4. **Batching**:
   - Batch messages theo symbol hoáº·c time window
   - Upload nhiá»u records vÃ o 1 file

5. **Monitoring**:
   - Lag monitoring (consumer lag)
   - Error rate tracking
   - Throughput metrics

---

## 7. ğŸ“Š Metrics & Monitoring

### 7.1 Key Metrics

**Producer Metrics:**
- Messages sent per second
- Error rate
- Latency (API â†’ Kafka)

**Kafka Metrics:**
- Messages in topic
- Consumer lag
- Partition size
- Retention usage

**Consumer Metrics:**
- Messages consumed per second
- Processing latency
- MinIO upload success rate
- Error rate

### 7.2 Monitoring Tools

1. **Kafdrop** (http://localhost:9000):
   - View topics, messages, consumer groups
   - Monitor consumer lag
   - Inspect message content

2. **Kafka CLI**:
```bash
# Check consumer lag
kafka-consumer-groups --bootstrap-server localhost:29092 \
  --group bronze-Consumer --describe

# View topic details
kafka-topics --bootstrap-server localhost:29092 \
  --topic stock-quotes --describe
```

---

## ğŸ“ TÃ³m Táº¯t

**Producer â†’ Consumer Flow:**
1. Producer fetch tá»« Finnhub API (5 symbols, má»—i 6s)
2. Enrich data (thÃªm symbol, fetched_at)
3. Serialize vÃ  gá»­i vÃ o Kafka topic `stock-quotes`
4. Consumer poll messages tá»« Kafka
5. Deserialize vÃ  lÆ°u vÃ o MinIO (key: `{symbol}/{timestamp}.json`)
6. Auto-commit offset

**Äáº·c Ä‘iá»ƒm:**
- **Throughput**: 50 messages/phÃºt
- **Latency**: ~200-750ms end-to-end
- **Reliability**: At-least-once delivery
- **Scalability**: CÃ³ thá»ƒ scale nhÆ°ng hiá»‡n táº¡i single partition

**Rá»§i ro:**
- Single partition â†’ khÃ´ng scale consumer
- No replication â†’ khÃ´ng fault-tolerant
- No error handling â†’ risk data loss
- No idempotency â†’ duplicate files

